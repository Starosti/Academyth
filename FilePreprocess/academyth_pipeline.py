import fitz  # PyMuPDF
import docx
import re
import spacy
from keybert import KeyBERT
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# === Ayarlar ===
model_name = "google/mt5-base"  # ok dilli T5, T羹rk癟e dahil
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# --- PDF/Word metin 癟覺karma ---

def extract_text_from_pdf(pdf_path: str) -> str:
    text = ""
    with fitz.open(pdf_path) as doc:
        for page in doc:
            text += page.get_text("text") + " "
    return text

def extract_text_from_docx(docx_path: str) -> str:
    doc = docx.Document(docx_path)
    text = " ".join(para.text for para in doc.paragraphs)
    return text

# --- Metin temizleme ---

def clean_text(text: str) -> str:
    text = text.replace("\n", " ")
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s.,?!]', '', text)
    return text.strip()

# --- C羹mlelere b繹lme ---

nlp = spacy.load("xx_sent_ud_sm")

def split_into_sentences(text: str) -> list[str]:
    doc = nlp(text)
    return [sent.text.strip() for sent in doc.sents]

# --- Keyword extraction ---

# kw_model = KeyBERT(model='xlm-r-distilroberta-base-paraphrase-v1')
# def extract_keywords(text: str, top_n=10):
#     keywords = kw_model.extract_keywords(
#         text,
#         keyphrase_ngram_range=(1, 2),
#         stop_words="english",
#         top_n=5
#     )

#     return keywords

def generate_question_from_sentence(sentence):
    input_text = f"generate question: {sentence}"
    inputs = tokenizer.encode(input_text, return_tensors="pt", max_length=512, truncation=True)

    outputs = model.generate(
        inputs,
        max_length=64,
        num_beams=4,
        early_stopping=True
    )
    question = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return question

# --- Ana pipeline ---

def process_document(file_path: str):
    # Dosyadan metin al
    if file_path.lower().endswith(".pdf"):
        raw_text = extract_text_from_pdf(file_path)
    elif file_path.lower().endswith(".docx"):
        raw_text = extract_text_from_docx(file_path)
    else:
        raise ValueError("Desteklenmeyen format. PDF veya DOCX olmal覺.")

    # Temizle
    cleaned = clean_text(raw_text)

    # C羹mlelere b繹l
    sentences = split_into_sentences(cleaned)

    # Keyword 癟覺kar
    #keywords = extract_keywords(cleaned, top_n=15)

    # TXT 癟覺kt覺s覺
    with open("output.txt", "w", encoding="utf-8") as f:
        f.write("=== CMLELER ===\n")
        for s in sentences:
            f.write(s + "\n")
        # f.write("\n=== ANAHTAR KEL襤MELER ===\n")
        # for kw, score in keywords:
        #     f.write(f"{kw} ({score:.4f})\n")

    print(f"襤lem tamamland覺! {len(sentences)} c羹mle bulundu. output.txt dosyas覺na yaz覺ld覺.")
    
    print(" mt5-base ile soru 羹retiliyor...")
    questions = []
    for sent in sentences[:20]:  # 襤lk 20 c羹mle ile s覺n覺rla, uzun s羹rebilir
        q = generate_question_from_sentence(sent)
        questions.append(q)

    with open("questions.txt", "w", encoding="utf-8") as f:
        for q in questions:
            f.write(q + "\n\n")

    print(f"{len(questions)} soru 羹retildi ve 'questions.txt' dosyas覺na yaz覺ld覺.")
    

if __name__ == "__main__":
    import sys
    if len(sys.argv) != 2:
        print("Kullan覺m: python academyth_pipeline.py <dosya_yolu>")
        sys.exit(1)

    process_document(sys.argv[1])
